{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"../data/processed/train_1.csv\")\n",
    "test = pd.read_csv(\"../data/processed/test_1.csv\")\n",
    "validation = pd.read_csv(\"../data/processed/validation_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = train['review']\n",
    "y = train['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-16 19:58:42,911 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-07-16 19:58:43,024 : INFO : collecting all words and their counts\n",
      "2018-07-16 19:58:43,027 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-16 19:58:43,065 : INFO : PROGRESS: at sentence #10000, processed 115672 words, keeping 4475 word types\n",
      "2018-07-16 19:58:43,084 : INFO : collected 5577 word types from a corpus of 183352 raw words and 15918 sentences\n",
      "2018-07-16 19:58:43,085 : INFO : Loading a fresh vocabulary\n",
      "2018-07-16 19:58:43,091 : INFO : min_count=40 retains 493 unique words (8% of original 5577, drops 5084)\n",
      "2018-07-16 19:58:43,092 : INFO : min_count=40 leaves 160490 word corpus (87% of original 183352, drops 22862)\n",
      "2018-07-16 19:58:43,096 : INFO : deleting the raw counts dictionary of 5577 items\n",
      "2018-07-16 19:58:43,098 : INFO : sample=0.001 downsamples 82 most-common words\n",
      "2018-07-16 19:58:43,101 : INFO : downsampling leaves estimated 98389 word corpus (61.3% of prior 160490)\n",
      "2018-07-16 19:58:43,105 : INFO : estimated required memory for 493 words and 300 dimensions: 1429700 bytes\n",
      "2018-07-16 19:58:43,106 : INFO : resetting layer weights\n",
      "2018-07-16 19:58:43,118 : INFO : training model with 4 workers on 493 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-16 19:58:43,459 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-16 19:58:43,463 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-16 19:58:43,465 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-16 19:58:43,488 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-16 19:58:43,489 : INFO : EPOCH - 1 : training on 183352 raw words (98401 effective words) took 0.3s, 293006 effective words/s\n",
      "2018-07-16 19:58:43,741 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-16 19:58:43,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-16 19:58:43,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-16 19:58:43,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-16 19:58:43,772 : INFO : EPOCH - 2 : training on 183352 raw words (98472 effective words) took 0.3s, 389734 effective words/s\n",
      "2018-07-16 19:58:44,011 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-16 19:58:44,031 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-16 19:58:44,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-16 19:58:44,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-16 19:58:44,038 : INFO : EPOCH - 3 : training on 183352 raw words (98516 effective words) took 0.2s, 403439 effective words/s\n",
      "2018-07-16 19:58:44,233 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-16 19:58:44,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-16 19:58:44,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-16 19:58:44,249 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-16 19:58:44,250 : INFO : EPOCH - 4 : training on 183352 raw words (98401 effective words) took 0.2s, 486249 effective words/s\n",
      "2018-07-16 19:58:44,465 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-16 19:58:44,473 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-16 19:58:44,482 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-16 19:58:44,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-16 19:58:44,494 : INFO : EPOCH - 5 : training on 183352 raw words (98309 effective words) took 0.2s, 417009 effective words/s\n",
      "2018-07-16 19:58:44,494 : INFO : training on a 916760 raw words (492099 effective words) took 1.4s, 358072 effective words/s\n",
      "2018-07-16 19:58:44,500 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-07-16 19:58:44,510 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-07-16 19:58:44,512 : INFO : not storing attribute vectors_norm\n",
      "2018-07-16 19:58:44,515 : INFO : not storing attribute cum_table\n",
      "2018-07-16 19:58:44,532 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(train['review'].apply(lambda x: x.split()), workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0187094 , -0.02739009, -0.01348962,  0.02048195, -0.00734926,\n",
       "       -0.04537932,  0.11759152, -0.02603204,  0.08739193, -0.00571861,\n",
       "       -0.04961926,  0.04553705,  0.06315306, -0.06911712, -0.07582539,\n",
       "        0.00601572,  0.03720866,  0.00787029,  0.03414796,  0.01396811,\n",
       "       -0.05495732,  0.01952831,  0.01075636,  0.00448649, -0.02663456,\n",
       "       -0.02846765, -0.02031692,  0.01705346,  0.06069159, -0.0536174 ,\n",
       "       -0.08966198,  0.0980629 ,  0.08366669,  0.0241677 , -0.02620028,\n",
       "        0.09934015,  0.02926669,  0.06896932,  0.11054508,  0.0857825 ,\n",
       "       -0.04380687, -0.05360619,  0.08893705,  0.03680015,  0.00671445,\n",
       "       -0.06334242, -0.01029712,  0.01846837,  0.0626487 , -0.13476504,\n",
       "       -0.03411483, -0.02727547,  0.09465968,  0.02500475, -0.02166574,\n",
       "        0.01463586,  0.03920843,  0.049907  ,  0.01668175,  0.08251037,\n",
       "       -0.06618717, -0.07928805, -0.14206052, -0.07374915, -0.08426023,\n",
       "        0.11673465,  0.03485214,  0.05841888, -0.02573732, -0.03050208,\n",
       "        0.04907488, -0.05424493, -0.10807415, -0.06244824, -0.00745491,\n",
       "       -0.03674061, -0.12485623,  0.00029376,  0.05677577,  0.0148495 ,\n",
       "       -0.06135026,  0.05714049,  0.0120484 , -0.10886376, -0.022064  ,\n",
       "        0.12196257,  0.01802211,  0.08902458,  0.00676698, -0.03115709,\n",
       "        0.0298676 ,  0.03270248,  0.01912519,  0.01910325,  0.04795611,\n",
       "        0.05812926,  0.03139453, -0.07480663,  0.00834099,  0.01496438,\n",
       "        0.07657149,  0.03949735,  0.04839164,  0.0593578 ,  0.00983192,\n",
       "       -0.08059094,  0.04204182,  0.07508925,  0.00467417,  0.04406524,\n",
       "        0.01749247, -0.00744155,  0.01928042, -0.11741884,  0.00117027,\n",
       "        0.03558737, -0.05736593,  0.06218939,  0.01119263,  0.04129567,\n",
       "        0.0075494 ,  0.12780571,  0.06562615, -0.08253594, -0.0069304 ,\n",
       "        0.00466034,  0.02413598,  0.07347074,  0.14204472, -0.054115  ,\n",
       "       -0.00796765,  0.01937019,  0.01255296,  0.03210051, -0.05348057,\n",
       "        0.07454424, -0.02040285,  0.03929213, -0.10195683, -0.00434676,\n",
       "       -0.02604881,  0.04345025, -0.02608225, -0.04699998, -0.00331052,\n",
       "        0.02423126,  0.00911272, -0.09420332, -0.02291187,  0.00381434,\n",
       "        0.05571257, -0.03677725,  0.04187486, -0.02641512,  0.02631902,\n",
       "       -0.02115864,  0.02115338, -0.00288749,  0.04786891, -0.04219143,\n",
       "        0.15805325, -0.10776934, -0.03569351,  0.05698248, -0.04758348,\n",
       "       -0.02614228, -0.00419226, -0.07391663, -0.02307669, -0.03956363,\n",
       "       -0.04986235,  0.03777852,  0.05479377, -0.01350161,  0.04358591,\n",
       "        0.00673382,  0.1153922 , -0.0186784 , -0.04213594, -0.0025295 ,\n",
       "        0.0010187 , -0.05977074, -0.08508664,  0.09059082, -0.05851531,\n",
       "       -0.03579646, -0.04314573,  0.0146354 , -0.09714492, -0.07460418,\n",
       "       -0.01083923,  0.01796448, -0.03489072,  0.02589666, -0.00470184,\n",
       "        0.03174735, -0.02919454, -0.0106585 ,  0.01033467, -0.03736976,\n",
       "       -0.07201136,  0.06808144,  0.06815942,  0.02774916,  0.11484137,\n",
       "       -0.04498621,  0.03322369,  0.00874894, -0.02831616,  0.040107  ,\n",
       "        0.11342864, -0.06589993,  0.02143709, -0.01920618, -0.13592215,\n",
       "        0.08358572,  0.09006713,  0.00768021,  0.00551904, -0.00470536,\n",
       "        0.05857297, -0.05099387,  0.05268756, -0.0231063 , -0.03938644,\n",
       "       -0.10297178,  0.02221014, -0.1138923 , -0.05423811,  0.0490933 ,\n",
       "       -0.00045658, -0.01637062, -0.01299079,  0.07269363,  0.02822157,\n",
       "        0.05210878,  0.10595547,  0.02389177, -0.11676791,  0.02764779,\n",
       "        0.06822688, -0.11499191,  0.07691961,  0.06543228,  0.07402118,\n",
       "       -0.08662249,  0.00672272,  0.07424551,  0.00478887,  0.05792822,\n",
       "       -0.04626031, -0.05709226, -0.09150273,  0.00205533, -0.09335912,\n",
       "        0.09380192,  0.02697366, -0.0227457 ,  0.0946926 , -0.03860718,\n",
       "       -0.04010889,  0.05989175, -0.04907141,  0.02540074,  0.0185029 ,\n",
       "       -0.01784419,  0.02332136,  0.11063184,  0.04944644,  0.00699733,\n",
       "        0.05358028,  0.04876625,  0.05768875, -0.00522822,  0.01605566,\n",
       "       -0.01748638, -0.15520096, -0.05791658,  0.04916661, -0.01679471,\n",
       "        0.00825161,  0.0179539 , -0.02911674, -0.00370706, -0.10970888,\n",
       "       -0.06527998, -0.0142151 , -0.0055978 , -0.09639974, -0.00495687,\n",
       "       -0.0106333 ,  0.05289865,  0.04766783, -0.01616333,  0.06467271,\n",
       "        0.00568325, -0.09052838,  0.04458989, -0.0298395 , -0.03930265],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['lazada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((1,num_features))\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    print(type(counter))\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features))\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        if counter%1000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "        reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model,num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "Review 0 of 12734\n",
      "Review 1000 of 12734\n",
      "Review 2000 of 12734\n",
      "Review 3000 of 12734\n",
      "Review 4000 of 12734\n",
      "Review 5000 of 12734\n",
      "Review 6000 of 12734\n",
      "Review 7000 of 12734\n",
      "Review 8000 of 12734\n",
      "Review 9000 of 12734\n",
      "Review 10000 of 12734\n",
      "Review 11000 of 12734\n",
      "Review 12000 of 12734\n",
      "Creating average feature vecs for test reviews\n",
      "<class 'float'>\n",
      "Review 0 of 3184\n",
      "Review 1000 of 3184\n",
      "Review 2000 of 3184\n",
      "Review 3000 of 3184\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = X_train.apply(lambda x: x.split())\n",
    "\n",
    "train_data_features = getAvgFeatureVecs(clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = X_test.apply(lambda x:x.split())\n",
    "\n",
    "test_data_features = getAvgFeatureVecs(clean_test_reviews, model, num_features )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

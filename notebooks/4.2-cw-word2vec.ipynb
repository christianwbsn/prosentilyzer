{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"../data/processed/train_1.csv\")\n",
    "test = pd.read_csv(\"../data/processed/test_1.csv\")\n",
    "validation = pd.read_csv(\"../data/processed/validation_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = train['review']\n",
    "y = train['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-11 21:45:11,998 : INFO : collecting all words and their counts\n",
      "2018-07-11 21:45:12,000 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-11 21:45:12,031 : INFO : PROGRESS: at sentence #10000, processed 115672 words, keeping 4475 word types\n",
      "2018-07-11 21:45:12,043 : INFO : collected 5577 word types from a corpus of 183352 raw words and 15918 sentences\n",
      "2018-07-11 21:45:12,044 : INFO : Loading a fresh vocabulary\n",
      "2018-07-11 21:45:12,048 : INFO : min_count=40 retains 493 unique words (8% of original 5577, drops 5084)\n",
      "2018-07-11 21:45:12,049 : INFO : min_count=40 leaves 160490 word corpus (87% of original 183352, drops 22862)\n",
      "2018-07-11 21:45:12,052 : INFO : deleting the raw counts dictionary of 5577 items\n",
      "2018-07-11 21:45:12,053 : INFO : sample=0.001 downsamples 82 most-common words\n",
      "2018-07-11 21:45:12,054 : INFO : downsampling leaves estimated 98389 word corpus (61.3% of prior 160490)\n",
      "2018-07-11 21:45:12,055 : INFO : estimated required memory for 493 words and 300 dimensions: 1429700 bytes\n",
      "2018-07-11 21:45:12,056 : INFO : resetting layer weights\n",
      "2018-07-11 21:45:12,072 : INFO : training model with 4 workers on 493 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-11 21:45:12,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-11 21:45:12,235 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-11 21:45:12,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-11 21:45:12,238 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-11 21:45:12,240 : INFO : EPOCH - 1 : training on 183352 raw words (98417 effective words) took 0.2s, 649513 effective words/s\n",
      "2018-07-11 21:45:12,386 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-11 21:45:12,392 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-11 21:45:12,393 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-11 21:45:12,399 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-11 21:45:12,400 : INFO : EPOCH - 2 : training on 183352 raw words (98735 effective words) took 0.1s, 678057 effective words/s\n",
      "2018-07-11 21:45:12,538 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-11 21:45:12,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-11 21:45:12,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-11 21:45:12,555 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-11 21:45:12,557 : INFO : EPOCH - 3 : training on 183352 raw words (98275 effective words) took 0.1s, 664847 effective words/s\n",
      "2018-07-11 21:45:12,707 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-11 21:45:12,714 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-11 21:45:12,724 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-11 21:45:12,725 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-11 21:45:12,726 : INFO : EPOCH - 4 : training on 183352 raw words (98056 effective words) took 0.2s, 614459 effective words/s\n",
      "2018-07-11 21:45:12,865 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-11 21:45:12,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-11 21:45:12,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-11 21:45:12,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-11 21:45:12,877 : INFO : EPOCH - 5 : training on 183352 raw words (98440 effective words) took 0.1s, 691528 effective words/s\n",
      "2018-07-11 21:45:12,882 : INFO : training on a 916760 raw words (491923 effective words) took 0.8s, 612752 effective words/s\n",
      "2018-07-11 21:45:12,891 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-07-11 21:45:12,901 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-07-11 21:45:12,902 : INFO : not storing attribute vectors_norm\n",
      "2018-07-11 21:45:12,902 : INFO : not storing attribute cum_table\n",
      "2018-07-11 21:45:12,917 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(train['review'].apply(lambda x: x.split()), workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kondisi', 0.9171394109725952),\n",
       " ('mulus', 0.8731058835983276),\n",
       " ('diterima', 0.8697145581245422),\n",
       " ('dan', 0.8541863560676575),\n",
       " ('keadaan', 0.8541389107704163),\n",
       " ('kemasan', 0.8493998050689697),\n",
       " ('aman', 0.8320677876472473),\n",
       " ('tanpa', 0.821256160736084),\n",
       " ('dalam', 0.811029851436615),\n",
       " ('telah', 0.806278645992279)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"baik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print(\"Review %d of %d\".format(counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review_to_wordlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1fefd1727c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclean_train_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'review_to_wordlist' is not defined"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tfidf..\n",
      "CPU times: user 158 ms, sys: 0 ns, total: 158 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "# print(\"Creating tfidf..\")\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# # bag of words tool.  \n",
    "# vectorizer = TfidfVectorizer(analyzer = \"word\",\n",
    "#                              tokenizer = None, \n",
    "#                              preprocessor = None,\n",
    "#                              stop_words = None, \n",
    "#                              max_features = 5000) \n",
    "\n",
    "# # fit_transform() does two functions: First, it fits the model\n",
    "# # and learns the vocabulary; second, it transforms our training data\n",
    "# # into feature vectors. The input to fit_transform should be a list of \n",
    "# # strings.\n",
    "# %time train_data_features = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Numpy arrays are easy to work with, so convert the result to an \n",
    "# # array\n",
    "# train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "CPU times: user 1min 1s, sys: 133 ms, total: 1min 1s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "%time forest = forest.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = vectorizer.transform(X_test)\n",
    "test_data_features = test_data_features.toarray()\n",
    "pred = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7710427135678392\n",
      "Confusion Matrix: [[ 579  497]\n",
      " [ 232 1876]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss,confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "acc = accuracy_score(y_test,pred)\n",
    "cm = confusion_matrix(y_test,pred)\n",
    "print(\"Accuracy Score: \" + str(acc))\n",
    "print(\"Confusion Matrix: \"+ str(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 145 ms, sys: 3.97 ms, total: 149 ms\n",
      "Wall time: 148 ms\n"
     ]
    }
   ],
   "source": [
    "# 1. import\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 2. instantiate a Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(train_data_features, y_train)\n",
    "pred = nb.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7584798994974874\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy Score: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy: 0.6620603015075377\n"
     ]
    }
   ],
   "source": [
    "null_ = []\n",
    "for i in range(0,len(y_test)):\n",
    "    null_.append(1)\n",
    "null_accuracy = accuracy_score(y_test, null_)\n",
    "print('Null accuracy:', null_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.88 s, sys: 27.8 ms, total: 5.91 s\n",
      "Wall time: 1.6 s\n",
      "Accuracy Score: 0.7751256281407035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nakama/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# 2. instantiate a Multinomial Naive Bayes model\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "%time lgbm.fit(train_data_features, y_train)\n",
    "pred = lgbm.predict(test_data_features)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy Score: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 296 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n",
      "Accuracy Score: 0.7550251256281407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nakama/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "xgbo = xgb.XGBClassifier()\n",
    "%time xgbo.fit(train_data_features, y_train)\n",
    "predic = xgbo.predict(test_data_features)\n",
    "acc = accuracy_score(y_test, predic)\n",
    "print(\"Accuracy Score: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 157 ms, sys: 5 Âµs, total: 157 ms\n",
      "Wall time: 157 ms\n",
      "Accuracy Score: 0.7713567839195979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression()\n",
    "%time logistic.fit(train_data_features, y_train)\n",
    "predic = logistic.predict(test_data_features)\n",
    "acc = accuracy_score(y_test, predic)\n",
    "print(\"Accuracy Score: \" + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
